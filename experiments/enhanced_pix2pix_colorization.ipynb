{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND CONFIGURATION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision.transforms import functional as TF\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"✓ Imports loaded\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created in: ./enhanced_pix2pix_project\n",
      "Device: cuda\n",
      "\n",
      "============================================================\n",
      "ENHANCED PIX2PIX CONFIGURATION\n",
      "============================================================\n",
      "Batch Size: 8\n",
      "Epochs: 50\n",
      "Image Size: 256x256\n",
      "Attention Enabled: True\n",
      "SE Blocks Enabled: True\n",
      "Multi-Scale Discriminator: 3 scales\n",
      "Perceptual Loss Weight: 10\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "class EnhancedConfig:\n",
    "    def __init__(self, test_mode=False):\n",
    "    \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "\n",
    "        self.BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
    "        self.EPOCHS = 50\n",
    "        self.LR = 0.0002\n",
    "        self.BETA1 = 0.5\n",
    "        self.BETA2 = 0.999\n",
    "        \n",
    "        # Loss weights\n",
    "        self.LAMBDA_L1 = 100\n",
    "        self.LAMBDA_PERCEPTUAL = 10  # Perceptual loss weight\n",
    "        self.LAMBDA_GP = 10  # Gradient penalty\n",
    "        \n",
    "        # Model parameters\n",
    "        self.IN_CHANNELS = 1\n",
    "        self.OUT_CHANNELS = 3\n",
    "        self.USE_ATTENTION = True  # Enable attention mechanisms\n",
    "        self.USE_SE_BLOCKS = True  # Enable Squeeze-and-Excitation\n",
    "        \n",
    "        # Data parameters\n",
    "        self.IMG_SIZE = 256\n",
    "        self.DATA_DIR = './pix2pix_dataset'\n",
    "        \n",
    "        # Multi-scale discriminator\n",
    "        self.NUM_D_SCALES = 3  # NEW: 3 discriminators at different scales\n",
    "        \n",
    "        # Training settings\n",
    "        self.SAVE_INTERVAL = 5\n",
    "        self.VAL_INTERVAL = 1\n",
    "        self.NUM_WORKERS = 4\n",
    "        self.PIN_MEMORY = True\n",
    "        self.USE_AMP = True\n",
    "        self.GRAD_CLIP = 1.0\n",
    "        self.WEIGHT_DECAY = 0.0001\n",
    "        \n",
    "        # Stability features\n",
    "        self.USE_LABEL_SMOOTHING = 0.1\n",
    "        self.ADD_INPUT_NOISE = 0.02\n",
    "        self.D_TRAIN_RATIO = 1\n",
    "        \n",
    "        # Learning rate scheduler (NEW)\n",
    "        self.USE_COSINE_ANNEALING = True\n",
    "        self.T_0 = 10  # Initial restart period\n",
    "        self.T_MULT = 2  # Period multiplier after restart\n",
    "        \n",
    "        # Paths\n",
    "        self.PROJECT_ROOT = './enhanced_pix2pix_project'\n",
    "        \n",
    "        if test_mode:\n",
    "            self.EXPERIMENT_NAME = f\"ENHANCED_TEST_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        else:\n",
    "            self.EXPERIMENT_NAME = f\"ENHANCED_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    def to_dict(self):\n",
    "        config_dict = {}\n",
    "        for key in dir(self):\n",
    "            if not key.startswith('__') and not callable(getattr(self, key)):\n",
    "                value = getattr(self, key)\n",
    "                if isinstance(value, torch.device):\n",
    "                    config_dict[key] = str(value)\n",
    "                else:\n",
    "                    config_dict[key] = value\n",
    "        return config_dict\n",
    "\n",
    "# Setup directories\n",
    "def setup_directories(config):\n",
    "    dirs = [\n",
    "        config.PROJECT_ROOT,\n",
    "        os.path.join(config.PROJECT_ROOT, 'checkpoints'),\n",
    "        os.path.join(config.PROJECT_ROOT, 'losses'),\n",
    "        os.path.join(config.PROJECT_ROOT, 'samples'),\n",
    "        os.path.join(config.PROJECT_ROOT, 'logs')\n",
    "    ]\n",
    "    \n",
    "    for dir_path in dirs:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Directories created in: {config.PROJECT_ROOT}\")\n",
    "    print(f\"Device: {config.device}\")\n",
    "    return config\n",
    "\n",
    "config = EnhancedConfig(test_mode=False)\n",
    "config = setup_directories(config)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENHANCED PIX2PIX CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Image Size: {config.IMG_SIZE}x{config.IMG_SIZE}\")\n",
    "print(f\"Attention Enabled: {config.USE_ATTENTION}\")\n",
    "print(f\"SE Blocks Enabled: {config.USE_SE_BLOCKS}\")\n",
    "print(f\"Multi-Scale Discriminator: {config.NUM_D_SCALES} scales\")\n",
    "print(f\"Perceptual Loss Weight: {config.LAMBDA_PERCEPTUAL}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION MECHANISMS\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # Use 1x1 convolutions to reduce computation\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        proj_query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key(x).view(batch_size, -1, width * height)\n",
    "        proj_value = self.value(x).view(batch_size, -1, width * height)\n",
    "        \n",
    "        # Attention map\n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(attention)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        \n",
    "        # Residual connection with learnable weight\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR WITH ATTENTION\n",
    "\n",
    "class EnhancedUNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0, use_se=False):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, \n",
    "                     stride=2, padding=1, bias=not normalize)\n",
    "        ]\n",
    "        \n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Add SE block if enabled\n",
    "        self.se_block = SEBlock(out_channels) if use_se else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        if self.se_block is not None:\n",
    "            x = self.se_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EnhancedUNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0, use_se=False):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                              kernel_size=4, stride=2, \n",
    "                              padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Add SE block if enabled\n",
    "        self.se_block = SEBlock(out_channels) if use_se else None\n",
    "    \n",
    "    def forward(self, x, skip_input=None):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        if self.se_block is not None:\n",
    "            x = self.se_block(x)\n",
    "        \n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class EnhancedGeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=3, use_attention=True, use_se=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.down1 = EnhancedUNetDown(in_channels, 64, normalize=False, use_se=use_se)\n",
    "        self.down2 = EnhancedUNetDown(64, 128, use_se=use_se)\n",
    "        self.down3 = EnhancedUNetDown(128, 256, use_se=use_se)\n",
    "        self.down4 = EnhancedUNetDown(256, 512, use_se=use_se)\n",
    "        self.down5 = EnhancedUNetDown(512, 512, use_se=use_se)\n",
    "        self.down6 = EnhancedUNetDown(512, 512, use_se=use_se)\n",
    "        self.down7 = EnhancedUNetDown(512, 512, use_se=use_se)\n",
    "        self.down8 = EnhancedUNetDown(512, 512, normalize=False, use_se=use_se)\n",
    "        \n",
    "        # Self-attention at bottleneck\n",
    "        self.attention = SelfAttention(512) if use_attention else nn.Identity()\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = EnhancedUNetUp(512, 512, dropout=0.5, use_se=use_se)\n",
    "        self.up2 = EnhancedUNetUp(1024, 512, dropout=0.5, use_se=use_se)\n",
    "        self.up3 = EnhancedUNetUp(1024, 512, dropout=0.5, use_se=use_se)\n",
    "        self.up4 = EnhancedUNetUp(1024, 512, dropout=0.0, use_se=use_se)\n",
    "        self.up5 = EnhancedUNetUp(1024, 256, dropout=0.0, use_se=use_se)\n",
    "        self.up6 = EnhancedUNetUp(512, 128, dropout=0.0, use_se=use_se)\n",
    "        self.up7 = EnhancedUNetUp(256, 64, dropout=0.0, use_se=use_se)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        \n",
    "        # Apply attention at bottleneck\n",
    "        d8 = self.attention(d8)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "        \n",
    "        return self.final(u7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-SCALE DISCRIMINATOR\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=4):  # 1 (gray) + 3 (RGB)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # C64\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # C128\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # C256\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # C512\n",
    "            nn.Conv2d(256, 512, 4, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, gray, rgb):\n",
    "        x = torch.cat([gray, rgb], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=3, num_scales=3):\n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        \n",
    "        # Create discriminators at different scales\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            PatchDiscriminator(in_channels + out_channels)\n",
    "            for _ in range(num_scales)\n",
    "        ])\n",
    "        \n",
    "        # Downsampling layer\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "    \n",
    "    def forward(self, gray, rgb):\n",
    "        outputs = []\n",
    "        \n",
    "        # Run through each discriminator at different scales\n",
    "        for i, discriminator in enumerate(self.discriminators):\n",
    "            outputs.append(discriminator(gray, rgb))\n",
    "            \n",
    "            # Downsample for next scale (except for last scale)\n",
    "            if i < self.num_scales - 1:\n",
    "                gray = self.downsample(gray)\n",
    "                rgb = self.downsample(rgb)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERCEPTUAL LOSS\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load pretrained VGG16\n",
    "        vgg = models.vgg16(pretrained=True).features\n",
    "        \n",
    "        # Use features from multiple layers\n",
    "        self.slice1 = nn.Sequential(*list(vgg[:4]))   # relu1_2\n",
    "        self.slice2 = nn.Sequential(*list(vgg[4:9]))  # relu2_2\n",
    "        self.slice3 = nn.Sequential(*list(vgg[9:16])) # relu3_3\n",
    "        self.slice4 = nn.Sequential(*list(vgg[16:23]))# relu4_3\n",
    "        \n",
    "        # Freeze VGG parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Normalization for ImageNet\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        # Denormalize from [-1, 1] to [0, 1]\n",
    "        x = (x + 1) / 2\n",
    "        # Normalize for VGG\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def forward(self, fake, real):\n",
    "        # Normalize inputs\n",
    "        fake = self.normalize(fake)\n",
    "        real = self.normalize(real)\n",
    "        \n",
    "        # Extract features\n",
    "        fake_features = []\n",
    "        real_features = []\n",
    "        \n",
    "        x_fake = fake\n",
    "        x_real = real\n",
    "        \n",
    "        for slice_layer in [self.slice1, self.slice2, self.slice3, self.slice4]:\n",
    "            x_fake = slice_layer(x_fake)\n",
    "            x_real = slice_layer(x_real)\n",
    "            fake_features.append(x_fake)\n",
    "            real_features.append(x_real)\n",
    "        \n",
    "        # Calculate loss across all layers\n",
    "        loss = 0\n",
    "        for fake_feat, real_feat in zip(fake_features, real_features):\n",
    "            loss += F.l1_loss(fake_feat, real_feat)\n",
    "        \n",
    "        return loss / len(fake_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def organize_dataset(grayscale_dir='dataset/grayscale', \n",
    "#                      colored_dir='dataset/colored',\n",
    "#                      output_root='pix2pix_dataset',\n",
    "#                      train_ratio=0.80,\n",
    "#                      val_ratio=0.15,\n",
    "#                      test_ratio=0.05):\n",
    "\n",
    "#     print(\"=\" * 60)\n",
    "#     print(\"ORGANIZING DATASET\")\n",
    "#     print(\"=\" * 60)\n",
    "#     print(f\"Source directories:\")\n",
    "#     print(f\"  Grayscale: {grayscale_dir}\")\n",
    "#     print(f\"  Colored:   {colored_dir}\")\n",
    "#     print(f\"\\nOutput root: {output_root}\")\n",
    "#     print(f\"Split ratios - Train: {train_ratio:.0%}, Val: {val_ratio:.0%}, Test: {test_ratio:.0%}\")\n",
    "#     print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "#     # Create output directory structure\n",
    "#     splits = ['train', 'val', 'test']\n",
    "#     for split in splits:\n",
    "#         os.makedirs(os.path.join(output_root, split, 'gray'), exist_ok=True)\n",
    "#         os.makedirs(os.path.join(output_root, split, 'rgb'), exist_ok=True)\n",
    "    \n",
    "#     # Get all image files from grayscale directory\n",
    "#     image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "#     gray_files = [f for f in os.listdir(grayscale_dir) \n",
    "#                   if f.lower().endswith(image_extensions)]\n",
    "    \n",
    "#     # Filter to only include files that have matching colored versions\n",
    "#     # Handle naming convention: chop_1_grey.png <-> chop_1_color.png\n",
    "#     valid_pairs = []\n",
    "#     missing_colored = []\n",
    "    \n",
    "#     print(\"Scanning for valid image pairs...\")\n",
    "#     for gray_file in tqdm(gray_files, desc=\"Processing\"):\n",
    "#         gray_path = os.path.join(grayscale_dir, gray_file)\n",
    "        \n",
    "#         # Convert grey filename to color filename\n",
    "#         # chop_1_grey.png -> chop_1_color.png\n",
    "#         if '_grey.' in gray_file:\n",
    "#             colored_file = gray_file.replace('_grey.', '_color.')\n",
    "#         elif '_gray.' in gray_file:\n",
    "#             colored_file = gray_file.replace('_gray.', '_color.')\n",
    "#         else:\n",
    "#             # If no _grey or _gray in filename, assume same filename\n",
    "#             colored_file = gray_file\n",
    "        \n",
    "#         colored_path = os.path.join(colored_dir, colored_file)\n",
    "        \n",
    "#         if os.path.exists(colored_path):\n",
    "#             valid_pairs.append({\n",
    "#                 'gray_file': gray_file,\n",
    "#                 'colored_file': colored_file\n",
    "#             })\n",
    "#         else:\n",
    "#             missing_colored.append(gray_file)\n",
    "    \n",
    "#     print(f\"\\nFound {len(gray_files)} grayscale images\")\n",
    "#     print(f\"Valid pairs (with matching colored): {len(valid_pairs)}\")\n",
    "    \n",
    "#     if missing_colored:\n",
    "#         print(f\"⚠ Missing colored versions: {len(missing_colored)}\")\n",
    "#         print(f\"  (These will be skipped)\")\n",
    "    \n",
    "#     if len(valid_pairs) == 0:\n",
    "#         raise ValueError(\"No valid image pairs found! Check your folder structure.\")\n",
    "    \n",
    "#     # Split data\n",
    "#     # First split: train vs (val+test)\n",
    "#     train_files, temp_files = train_test_split(\n",
    "#         valid_pairs, \n",
    "#         test_size=(val_ratio + test_ratio),\n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # Second split: val vs test\n",
    "#     val_size = val_ratio / (val_ratio + test_ratio)\n",
    "#     val_files, test_files = train_test_split(\n",
    "#         temp_files,\n",
    "#         test_size=(1 - val_size),\n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"SPLIT SUMMARY\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     print(f\"Train: {len(train_files):4d} images ({len(train_files)/len(valid_pairs)*100:.1f}%)\")\n",
    "#     print(f\"Val:   {len(val_files):4d} images ({len(val_files)/len(valid_pairs)*100:.1f}%)\")\n",
    "#     print(f\"Test:  {len(test_files):4d} images ({len(test_files)/len(valid_pairs)*100:.1f}%)\")\n",
    "#     print(f\"Total: {len(valid_pairs):4d} images\")\n",
    "#     print(f\"{'='*60}\\n\")\n",
    "    \n",
    "#     # Copy files to splits\n",
    "#     def copy_split_files(file_list, split_name):\n",
    "#         gray_out = os.path.join(output_root, split_name, 'gray')\n",
    "#         rgb_out = os.path.join(output_root, split_name, 'rgb')\n",
    "        \n",
    "#         for pair in tqdm(file_list, desc=f\"Copying {split_name:5s} files\"):\n",
    "#             gray_file = pair['gray_file']\n",
    "#             colored_file = pair['colored_file']\n",
    "            \n",
    "#             # Create a common base name for both files (remove _grey and _color suffixes)\n",
    "#             # chop_1_grey.png -> chop_1.png\n",
    "#             if '_grey.' in gray_file:\n",
    "#                 base_name = gray_file.replace('_grey.', '.')\n",
    "#             elif '_gray.' in gray_file:\n",
    "#                 base_name = gray_file.replace('_gray.', '.')\n",
    "#             else:\n",
    "#                 base_name = gray_file\n",
    "            \n",
    "#             # Copy grayscale\n",
    "#             shutil.copy2(\n",
    "#                 os.path.join(grayscale_dir, gray_file),\n",
    "#                 os.path.join(gray_out, base_name)\n",
    "#             )\n",
    "#             # Copy colored\n",
    "#             shutil.copy2(\n",
    "#                 os.path.join(colored_dir, colored_file),\n",
    "#                 os.path.join(rgb_out, base_name)\n",
    "#             )\n",
    "    \n",
    "#     copy_split_files(train_files, 'train')\n",
    "#     copy_split_files(val_files, 'val')\n",
    "#     copy_split_files(test_files, 'test')\n",
    "    \n",
    "#     # Save dataset statistics\n",
    "#     stats = {\n",
    "#         'total_pairs': len(valid_pairs),\n",
    "#         'train': len(train_files),\n",
    "#         'val': len(val_files),\n",
    "#         'test': len(test_files),\n",
    "#         'missing_colored': len(missing_colored),\n",
    "#         'split_ratios': {\n",
    "#             'train': train_ratio,\n",
    "#             'val': val_ratio,\n",
    "#             'test': test_ratio\n",
    "#         },\n",
    "#         'source_dirs': {\n",
    "#             'grayscale': grayscale_dir,\n",
    "#             'colored': colored_dir\n",
    "#         },\n",
    "#         'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     }\n",
    "    \n",
    "#     stats_path = os.path.join(output_root, 'dataset_stats.json')\n",
    "#     with open(stats_path, 'w') as f:\n",
    "#         json.dump(stats, f, indent=2)\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"✓ DATASET ORGANIZATION COMPLETE\")\n",
    "  \n",
    "    \n",
    "#     return output_root, stats\n",
    "\n",
    "\n",
    "\n",
    "# dataset_path, stats = organize_dataset(\n",
    "#         grayscale_dir='dataset/grayscale',\n",
    "#         colored_dir='dataset/colored',\n",
    "#         output_root='pix2pix_dataset',\n",
    "#         train_ratio=0.80,\n",
    "#         val_ratio=0.15,\n",
    "#         test_ratio=0.05\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPairedDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', config=None, use_percentage=1.0):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.config = config\n",
    "        self.use_percentage = use_percentage\n",
    "        \n",
    "        # Get all image pairs\n",
    "        self.pairs = []\n",
    "        gray_dir = os.path.join(root_dir, mode, 'gray')\n",
    "        rgb_dir = os.path.join(root_dir, mode, 'rgb')\n",
    "        \n",
    "        # Collect all valid pairs\n",
    "        all_pairs = []\n",
    "        for img_name in os.listdir(gray_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                gray_path = os.path.join(gray_dir, img_name)\n",
    "                rgb_path = os.path.join(rgb_dir, img_name)\n",
    "                \n",
    "                if os.path.exists(rgb_path):\n",
    "                    all_pairs.append((gray_path, rgb_path))\n",
    "        \n",
    "        # Shuffle and select percentage\n",
    "        np.random.shuffle(all_pairs)\n",
    "        n_total = len(all_pairs)\n",
    "        n_select = int(n_total * use_percentage)\n",
    "        self.pairs = all_pairs[:n_select]\n",
    "        \n",
    "        print(f\"{mode.capitalize()} dataset: {len(self.pairs)}/{n_total} pairs ({use_percentage*100:.1f}%)\")\n",
    "        \n",
    "        self.transform = self._get_transforms()\n",
    "    \n",
    "    def _get_transforms(self):\n",
    "        img_size = self.config.IMG_SIZE if self.config else 256\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'gray': transforms.Compose([\n",
    "                    transforms.Lambda(lambda img: self.center_crop_to_square(img)),  \n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomVerticalFlip(p=0.2),\n",
    "                    transforms.RandomRotation(degrees=10),\n",
    "                    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                ]),\n",
    "                'rgb': transforms.Compose([\n",
    "                    transforms.Lambda(lambda img: self.center_crop_to_square(img)),  \n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomVerticalFlip(p=0.2),\n",
    "                    transforms.RandomRotation(degrees=10),\n",
    "                    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "                    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'gray': transforms.Compose([\n",
    "                    transforms.Lambda(lambda img: self.center_crop_to_square(img)),  \n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                ]),\n",
    "                'rgb': transforms.Compose([\n",
    "                    transforms.Lambda(lambda img: self.center_crop_to_square(img)),  \n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "            }\n",
    "    \n",
    "    def center_crop_to_square(self, img):\n",
    "        width, height = img.size\n",
    "        \n",
    "        if width == height:\n",
    "            return img\n",
    "        \n",
    "        # Crop to the smaller dimension\n",
    "        crop_size = min(width, height)\n",
    "        \n",
    "        # Calculate crop coordinates (center crop)\n",
    "        left = (width - crop_size) // 2\n",
    "        top = (height - crop_size) // 2\n",
    "        right = left + crop_size\n",
    "        bottom = top + crop_size\n",
    "        \n",
    "        return img.crop((left, top, right, bottom))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gray_path, rgb_path = self.pairs[idx]\n",
    "        \n",
    "        # Apply same random seed for paired augmentation\n",
    "        seed = np.random.randint(2147483647)\n",
    "        \n",
    "        gray_img = Image.open(gray_path).convert('L')\n",
    "        rgb_img = Image.open(rgb_path).convert('RGB')\n",
    "        \n",
    "        # Set random seed for reproducible augmentation\n",
    "        torch.manual_seed(seed)\n",
    "        gray_tensor = self.transform['gray'](gray_img)\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        rgb_tensor = self.transform['rgb'](rgb_img)\n",
    "        \n",
    "        return gray_tensor, rgb_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Enhanced Trainer defined\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED TRAINER\n",
    "\n",
    "class EnhancedPix2PixTrainer:\n",
    "    def __init__(self, generator, discriminator, config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Models\n",
    "        self.generator = generator.to(self.device)\n",
    "        self.discriminator = discriminator.to(self.device)\n",
    "        \n",
    "        # Losses\n",
    "        self.criterion_gan = nn.BCEWithLogitsLoss()\n",
    "        self.criterion_l1 = nn.L1Loss()\n",
    "        self.perceptual_loss = PerceptualLoss().to(self.device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer_G = optim.Adam(\n",
    "            self.generator.parameters(), \n",
    "            lr=config.LR, \n",
    "            betas=(config.BETA1, config.BETA2),\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        self.optimizer_D = optim.Adam(\n",
    "            self.discriminator.parameters(), \n",
    "            lr=config.LR, \n",
    "            betas=(config.BETA1, config.BETA2),\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # AMP GradScalers\n",
    "        self.scaler_G = GradScaler(enabled=config.USE_AMP)\n",
    "        self.scaler_D = GradScaler(enabled=config.USE_AMP)\n",
    "        \n",
    "        # Cosine Annealing with Warm Restarts\n",
    "        if config.USE_COSINE_ANNEALING:\n",
    "            self.scheduler_G = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                self.optimizer_G, T_0=config.T_0, T_mult=config.T_MULT\n",
    "            )\n",
    "            self.scheduler_D = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                self.optimizer_D, T_0=config.T_0, T_mult=config.T_MULT\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer_G, mode='min', factor=0.5, patience=10\n",
    "            )\n",
    "            self.scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer_D, mode='min', factor=0.5, patience=10\n",
    "            )\n",
    "        \n",
    "        # Tracking\n",
    "        self.current_epoch = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.losses = {\n",
    "            'train': {'D': [], 'G': [], 'G_GAN': [], 'G_L1': [], 'G_Perceptual': []},\n",
    "            'val': {'D': [], 'G': [], 'G_GAN': [], 'G_L1': [], 'G_Perceptual': []}\n",
    "        }\n",
    "        \n",
    "        # Early stopping\n",
    "        self.patience = 50\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        # Create experiment directory\n",
    "        self.exp_dir = os.path.join(config.PROJECT_ROOT, config.EXPERIMENT_NAME)\n",
    "        self.checkpoint_dir = os.path.join(self.exp_dir, 'checkpoints')\n",
    "        self.samples_dir = os.path.join(self.exp_dir, 'samples')\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.samples_dir, exist_ok=True)\n",
    "        \n",
    "        # Save config\n",
    "        self._save_config()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Experiment directory: {self.exp_dir}\")\n",
    "        print(f\"Using AMP: {config.USE_AMP}\")\n",
    "        print(f\"Using Cosine Annealing: {config.USE_COSINE_ANNEALING}\")\n",
    "        print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    def _save_config(self):\n",
    "        config_dict = self.config.to_dict()\n",
    "        with open(os.path.join(self.exp_dir, 'config.json'), 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    def set_requires_grad(self, model, requires_grad):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def gradient_penalty(self, real_gray, real_rgb, fake_rgb):\n",
    "        batch_size = real_gray.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1, device=self.device)\n",
    "        interpolated = (alpha * real_rgb + (1 - alpha) * fake_rgb).requires_grad_(True)\n",
    "        \n",
    "        # Get discriminator outputs at all scales\n",
    "        d_interpolated_list = self.discriminator(real_gray, interpolated)\n",
    "        \n",
    "        gp_loss = 0\n",
    "        for d_interpolated in d_interpolated_list:\n",
    "            gradients = torch.autograd.grad(\n",
    "                outputs=d_interpolated,\n",
    "                inputs=interpolated,\n",
    "                grad_outputs=torch.ones_like(d_interpolated),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "            \n",
    "            gradients = gradients.view(gradients.size(0), -1)\n",
    "            gp_loss += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        \n",
    "        return gp_loss / len(d_interpolated_list)\n",
    "    \n",
    "    def train_step(self, real_gray, real_rgb):\n",
    "        real_gray, real_rgb = real_gray.to(self.device), real_rgb.to(self.device)\n",
    "        batch_size = real_gray.size(0)\n",
    "        \n",
    "        # Label smoothing\n",
    "        smooth = self.config.USE_LABEL_SMOOTHING\n",
    "        \n",
    "        # Add input noise\n",
    "        if self.config.ADD_INPUT_NOISE > 0:\n",
    "            noise_level = self.config.ADD_INPUT_NOISE\n",
    "            real_rgb_noisy = real_rgb + torch.randn_like(real_rgb) * noise_level\n",
    "        else:\n",
    "            real_rgb_noisy = real_rgb\n",
    "        \n",
    "        # ===== Train Discriminator =====\n",
    "        self.set_requires_grad(self.discriminator, True)\n",
    "        self.optimizer_D.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=self.config.USE_AMP):\n",
    "            fake_rgb = self.generator(real_gray)\n",
    "            \n",
    "            # Multi-scale discriminator outputs\n",
    "            pred_real_list = self.discriminator(real_gray, real_rgb_noisy)\n",
    "            pred_fake_list = self.discriminator(real_gray, fake_rgb.detach())\n",
    "            \n",
    "            # Calculate GAN loss for each scale\n",
    "            loss_D_real = 0\n",
    "            loss_D_fake = 0\n",
    "            \n",
    "            for pred_real, pred_fake in zip(pred_real_list, pred_fake_list):\n",
    "                real_labels = torch.ones_like(pred_real) * (1 - smooth)\n",
    "                fake_labels = torch.zeros_like(pred_fake) + smooth\n",
    "                \n",
    "                loss_D_real += self.criterion_gan(pred_real, real_labels)\n",
    "                loss_D_fake += self.criterion_gan(pred_fake, fake_labels)\n",
    "            \n",
    "            loss_D_real = loss_D_real / len(pred_real_list)\n",
    "            loss_D_fake = loss_D_fake / len(pred_fake_list)\n",
    "            \n",
    "            # Gradient penalty\n",
    "            gp = self.gradient_penalty(real_gray, real_rgb, fake_rgb)\n",
    "            \n",
    "            loss_D = loss_D_real + loss_D_fake + self.config.LAMBDA_GP * gp\n",
    "        \n",
    "        self.scaler_D.scale(loss_D).backward()\n",
    "        \n",
    "        if self.config.GRAD_CLIP > 0:\n",
    "            self.scaler_D.unscale_(self.optimizer_D)\n",
    "            torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), self.config.GRAD_CLIP)\n",
    "        \n",
    "        self.scaler_D.step(self.optimizer_D)\n",
    "        self.scaler_D.update()\n",
    "        \n",
    "        # ===== Train Generator =====\n",
    "        self.set_requires_grad(self.discriminator, False)\n",
    "        self.optimizer_G.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=self.config.USE_AMP):\n",
    "            fake_rgb = self.generator(real_gray)\n",
    "            \n",
    "            # Multi-scale GAN loss\n",
    "            pred_fake_list = self.discriminator(real_gray, fake_rgb)\n",
    "            \n",
    "            loss_G_gan = 0\n",
    "            for pred_fake in pred_fake_list:\n",
    "                real_labels = torch.ones_like(pred_fake)\n",
    "                loss_G_gan += self.criterion_gan(pred_fake, real_labels)\n",
    "            loss_G_gan = loss_G_gan / len(pred_fake_list)\n",
    "            \n",
    "            # L1 loss\n",
    "            loss_G_l1 = self.criterion_l1(fake_rgb, real_rgb)\n",
    "            \n",
    "            # Perceptual loss\n",
    "            loss_G_perceptual = self.perceptual_loss(fake_rgb, real_rgb)\n",
    "            \n",
    "            # Total generator loss\n",
    "            loss_G = loss_G_gan + \\\n",
    "                     self.config.LAMBDA_L1 * loss_G_l1 + \\\n",
    "                     self.config.LAMBDA_PERCEPTUAL * loss_G_perceptual\n",
    "        \n",
    "        self.scaler_G.scale(loss_G).backward()\n",
    "        \n",
    "        if self.config.GRAD_CLIP > 0:\n",
    "            self.scaler_G.unscale_(self.optimizer_G)\n",
    "            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), self.config.GRAD_CLIP)\n",
    "        \n",
    "        self.scaler_G.step(self.optimizer_G)\n",
    "        self.scaler_G.update()\n",
    "        \n",
    "        return {\n",
    "            'D': loss_D.item(),\n",
    "            'G': loss_G.item(),\n",
    "            'G_GAN': loss_G_gan.item(),\n",
    "            'G_L1': loss_G_l1.item(),\n",
    "            'G_Perceptual': loss_G_perceptual.item()\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader):\n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "        val_losses = {'D': [], 'G': [], 'G_GAN': [], 'G_L1': [], 'G_Perceptual': []}\n",
    "        \n",
    "        for real_gray, real_rgb in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            real_gray, real_rgb = real_gray.to(self.device), real_rgb.to(self.device)\n",
    "            \n",
    "            fake_rgb = self.generator(real_gray)\n",
    "            \n",
    "            # Multi-scale discriminator\n",
    "            pred_real_list = self.discriminator(real_gray, real_rgb)\n",
    "            pred_fake_list = self.discriminator(real_gray, fake_rgb)\n",
    "            \n",
    "            # D loss\n",
    "            loss_D_real = sum([self.criterion_gan(pred, torch.ones_like(pred)) \n",
    "                              for pred in pred_real_list]) / len(pred_real_list)\n",
    "            loss_D_fake = sum([self.criterion_gan(pred, torch.zeros_like(pred)) \n",
    "                              for pred in pred_fake_list]) / len(pred_fake_list)\n",
    "            loss_D = loss_D_real + loss_D_fake\n",
    "            \n",
    "            # G losses\n",
    "            loss_G_gan = sum([self.criterion_gan(pred, torch.ones_like(pred)) \n",
    "                             for pred in pred_fake_list]) / len(pred_fake_list)\n",
    "            loss_G_l1 = self.criterion_l1(fake_rgb, real_rgb)\n",
    "            loss_G_perceptual = self.perceptual_loss(fake_rgb, real_rgb)\n",
    "            loss_G = loss_G_gan + self.config.LAMBDA_L1 * loss_G_l1 + \\\n",
    "                     self.config.LAMBDA_PERCEPTUAL * loss_G_perceptual\n",
    "            \n",
    "            val_losses['D'].append(loss_D.item())\n",
    "            val_losses['G'].append(loss_G.item())\n",
    "            val_losses['G_GAN'].append(loss_G_gan.item())\n",
    "            val_losses['G_L1'].append(loss_G_l1.item())\n",
    "            val_losses['G_Perceptual'].append(loss_G_perceptual.item())\n",
    "        \n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        return {k: np.mean(v) for k, v in val_losses.items()}\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"STARTING ENHANCED TRAINING\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for epoch in range(self.current_epoch, self.config.EPOCHS):\n",
    "            self.current_epoch = epoch + 1\n",
    "            \n",
    "            # Training\n",
    "            self.generator.train()\n",
    "            self.discriminator.train()\n",
    "            \n",
    "            train_losses = {'D': [], 'G': [], 'G_GAN': [], 'G_L1': [], 'G_Perceptual': []}\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {self.current_epoch}/{self.config.EPOCHS}\")\n",
    "            for batch_idx, (real_gray, real_rgb) in enumerate(pbar):\n",
    "                losses = self.train_step(real_gray, real_rgb)\n",
    "                \n",
    "                for k, v in losses.items():\n",
    "                    train_losses[k].append(v)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'D': f\"{losses['D']:.4f}\",\n",
    "                    'G': f\"{losses['G']:.4f}\",\n",
    "                    'L1': f\"{losses['G_L1']:.4f}\",\n",
    "                    'Perceptual': f\"{losses['G_Perceptual']:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Calculate epoch averages\n",
    "            avg_train_losses = {k: np.mean(v) for k, v in train_losses.items()}\n",
    "            for k, v in avg_train_losses.items():\n",
    "                self.losses['train'][k].append(v)\n",
    "            \n",
    "            # Validation\n",
    "            if val_loader and (self.current_epoch % self.config.VAL_INTERVAL == 0):\n",
    "                avg_val_losses = self.validate(val_loader)\n",
    "                for k, v in avg_val_losses.items():\n",
    "                    self.losses['val'][k].append(v)\n",
    "                \n",
    "                print(f\"\\nEpoch {self.current_epoch} - Val Loss: {avg_val_losses['G']:.4f} \"\n",
    "                      f\"(L1: {avg_val_losses['G_L1']:.4f}, Perceptual: {avg_val_losses['G_Perceptual']:.4f})\")\n",
    "                \n",
    "                # Save best model\n",
    "                if avg_val_losses['G'] < self.best_val_loss:\n",
    "                    self.best_val_loss = avg_val_losses['G']\n",
    "                    self.best_epoch = self.current_epoch\n",
    "                    self.save_checkpoint('best')\n",
    "                    print(f\"✓ New best model saved! (Val Loss: {self.best_val_loss:.4f})\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if self.current_epoch - self.best_epoch > self.patience:\n",
    "                    print(f\"\\nEarly stopping triggered. No improvement for {self.patience} epochs.\")\n",
    "                    break\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            if self.config.USE_COSINE_ANNEALING:\n",
    "                self.scheduler_G.step()\n",
    "                self.scheduler_D.step()\n",
    "            else:\n",
    "                if val_loader:\n",
    "                    self.scheduler_G.step(avg_val_losses['G'])\n",
    "                    self.scheduler_D.step(avg_val_losses['D'])\n",
    "            \n",
    "            # Periodic checkpoint\n",
    "            if self.current_epoch % self.config.SAVE_INTERVAL == 0:\n",
    "                self.save_checkpoint()\n",
    "                \n",
    "                # Generate samples\n",
    "                if batch_idx > 0:\n",
    "                    self.generate_samples(real_gray, real_rgb, self.current_epoch)\n",
    "            \n",
    "            # Save losses\n",
    "            self.save_losses()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETE!\")\n",
    "        print(f\"Best epoch: {self.best_epoch}\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.save_checkpoint('final')\n",
    "        self.plot_losses()\n",
    "    \n",
    "    def save_checkpoint(self, name=None):\n",
    "        if name is None:\n",
    "            name = f'epoch_{self.current_epoch:04d}'\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': self.optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': self.optimizer_D.state_dict(),\n",
    "            'scheduler_G_state_dict': self.scheduler_G.state_dict(),\n",
    "            'scheduler_D_state_dict': self.scheduler_D.state_dict(),\n",
    "            'scaler_G_state_dict': self.scaler_G.state_dict(),\n",
    "            'scaler_D_state_dict': self.scaler_D.state_dict(),\n",
    "            'losses': self.losses,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config.to_dict()\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(self.checkpoint_dir, f'{name}.pth')\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Checkpoint saved: {path}\")\n",
    "    \n",
    "    def save_losses(self):\n",
    "        losses_path = os.path.join(self.exp_dir, 'losses.json')\n",
    "        with open(losses_path, 'w') as f:\n",
    "            json.dump(self.losses, f, indent=2)\n",
    "    \n",
    "    def generate_samples(self, real_gray, real_rgb, epoch, num_samples=4):\n",
    "        self.generator.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            real_gray = real_gray[:num_samples].to(self.device)\n",
    "            real_rgb = real_rgb[:num_samples].to(self.device)\n",
    "            fake_rgb = self.generator(real_gray)\n",
    "            \n",
    "            real_gray_np = real_gray.cpu().numpy()\n",
    "            real_rgb_np = real_rgb.cpu().numpy()\n",
    "            fake_rgb_np = fake_rgb.cpu().numpy()\n",
    "        \n",
    "        def denormalize(img):\n",
    "            return (img.transpose(0, 2, 3, 1) * 0.5 + 0.5).clip(0, 1)\n",
    "        \n",
    "        fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Input\n",
    "            axes[i, 0].imshow(real_gray_np[i, 0], cmap='gray')\n",
    "            axes[i, 0].set_title('Input (Grayscale)')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Generated\n",
    "            axes[i, 1].imshow(denormalize(fake_rgb_np)[i])\n",
    "            axes[i, 1].set_title('Generated (RGB)')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Ground Truth\n",
    "            axes[i, 2].imshow(denormalize(real_rgb_np)[i])\n",
    "            axes[i, 2].set_title('Ground Truth (RGB)')\n",
    "            axes[i, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        sample_path = os.path.join(self.samples_dir, f'samples_epoch_{epoch:04d}.png')\n",
    "        plt.savefig(sample_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        self.generator.train()\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        epochs = range(1, len(self.losses['train']['G']) + 1)\n",
    "        \n",
    "        # Generator vs Discriminator (Train)\n",
    "        axes[0, 0].plot(epochs, self.losses['train']['G'], label='Generator', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, self.losses['train']['D'], label='Discriminator', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Generator vs Discriminator (Train)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Generator Components (Train)\n",
    "        axes[0, 1].plot(epochs, self.losses['train']['G_GAN'], label='GAN Loss', linewidth=2)\n",
    "        axes[0, 1].plot(epochs, self.losses['train']['G_L1'], label='L1 Loss', linewidth=2)\n",
    "        axes[0, 1].plot(epochs, self.losses['train']['G_Perceptual'], label='Perceptual Loss', linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].set_title('Generator Loss Components (Train)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # L1 Loss Detail\n",
    "        axes[0, 2].plot(epochs, self.losses['train']['G_L1'], linewidth=2, color='green')\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('L1 Loss')\n",
    "        axes[0, 2].set_title('L1 Loss (Train)')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        if self.losses['val']['G']:\n",
    "            val_epochs = range(1, len(self.losses['val']['G']) + 1)\n",
    "            \n",
    "            # Validation losses\n",
    "            axes[1, 0].plot(val_epochs, self.losses['val']['G'], label='Generator', linewidth=2)\n",
    "            axes[1, 0].plot(val_epochs, self.losses['val']['D'], label='Discriminator', linewidth=2)\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Loss')\n",
    "            axes[1, 0].set_title('Generator vs Discriminator (Validation)')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[1, 1].plot(val_epochs, self.losses['val']['G_GAN'], label='GAN Loss', linewidth=2)\n",
    "            axes[1, 1].plot(val_epochs, self.losses['val']['G_L1'], label='L1 Loss', linewidth=2)\n",
    "            axes[1, 1].plot(val_epochs, self.losses['val']['G_Perceptual'], label='Perceptual Loss', linewidth=2)\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Loss')\n",
    "            axes[1, 1].set_title('Generator Loss Components (Validation)')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[1, 2].plot(val_epochs, self.losses['val']['G_Perceptual'], linewidth=2, color='red')\n",
    "            axes[1, 2].set_xlabel('Epoch')\n",
    "            axes[1, 2].set_ylabel('Perceptual Loss')\n",
    "            axes[1, 2].set_title('Perceptual Loss (Validation)')\n",
    "            axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        loss_plot_path = os.path.join(self.exp_dir, 'loss_curves.png')\n",
    "        plt.savefig(loss_plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"\\nLoss curves saved: {loss_plot_path}\")\n",
    "\n",
    "print(\"✓ Enhanced Trainer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING MODELS\n",
      "============================================================\n",
      "Generator parameters: 55,057,220\n",
      "Discriminator parameters: 8,294,595\n",
      "Total parameters: 63,351,815\n",
      "\n",
      "============================================================\n",
      "LOADING DATASETS\n",
      "============================================================\n",
      "Train dataset: 1369/6847 pairs (20.0%)\n",
      "Val dataset: 1283/1283 pairs (100.0%)\n",
      "✓ Training batches: 171\n",
      "✓ Validation batches: 161\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#INITIALIZE MODELS AND DATALOADERS\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING MODELS\")\n",
    "print(\"=\"*60)\n",
    "num_workers = 0 if os.name == 'nt' else min(config.NUM_WORKERS, 2)\n",
    "\n",
    "# Initialize models\n",
    "generator = EnhancedGeneratorUNet(\n",
    "    in_channels=config.IN_CHANNELS,\n",
    "    out_channels=config.OUT_CHANNELS,\n",
    "    use_attention=config.USE_ATTENTION,\n",
    "    use_se=config.USE_SE_BLOCKS\n",
    ")\n",
    "\n",
    "discriminator = MultiScaleDiscriminator(\n",
    "    in_channels=config.IN_CHANNELS,\n",
    "    out_channels=config.OUT_CHANNELS,\n",
    "    num_scales=config.NUM_D_SCALES\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "g_params = sum(p.numel() for p in generator.parameters())\n",
    "d_params = sum(p.numel() for p in discriminator.parameters())\n",
    "\n",
    "print(f\"Generator parameters: {g_params:,}\")\n",
    "print(f\"Discriminator parameters: {d_params:,}\")\n",
    "print(f\"Total parameters: {g_params + d_params:,}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_dataset = EnhancedPairedDataset(\n",
    "    root_dir=config.DATA_DIR,\n",
    "    mode='train',\n",
    "    config=config,\n",
    "    use_percentage=0.2\n",
    ")\n",
    "\n",
    "val_dataset = EnhancedPairedDataset(\n",
    "    root_dir=config.DATA_DIR,\n",
    "    mode='val',\n",
    "    config=config,\n",
    "    use_percentage=1.0\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=config.PIN_MEMORY,\n",
    "    num_workers=num_workers,  \n",
    "    persistent_workers=False, \n",
    "    drop_last=True  # (train only)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,  \n",
    "    persistent_workers=False, \n",
    "    pin_memory=config.PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(f\"✓ Training batches: {len(train_loader)}\")\n",
    "print(f\"✓ Validation batches: {len(val_loader)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment directory: ./enhanced_pix2pix_project\\ENHANCED_20251225_203059\n",
      "Using AMP: True\n",
      "Using Cosine Annealing: True\n",
      "Batch size: 8\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STARTING ENHANCED TRAINING\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 171/171 [03:59<00:00,  1.40s/it, D=2312.3518, G=16.5275, L1=0.1055, Perceptual=0.5287]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Val Loss: 19.4316 (L1: 0.1243, Perceptual: 0.6305)\n",
      "Checkpoint saved: ./enhanced_pix2pix_project\\ENHANCED_20251225_203059\\checkpoints\\best.pth\n",
      "✓ New best model saved! (Val Loss: 19.4316)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 171/171 [06:31<00:00,  2.29s/it, D=2853.8835, G=13.8212, L1=0.0871, Perceptual=0.4419]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - Val Loss: 12.6130 (L1: 0.0750, Perceptual: 0.4418)\n",
      "Checkpoint saved: ./enhanced_pix2pix_project\\ENHANCED_20251225_203059\\checkpoints\\best.pth\n",
      "✓ New best model saved! (Val Loss: 12.6130)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 171/171 [06:25<00:00,  2.26s/it, D=5468.8423, G=12.4780, L1=0.0829, Perceptual=0.3497]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 - Val Loss: 11.9520 (L1: 0.0723, Perceptual: 0.4028)\n",
      "Checkpoint saved: ./enhanced_pix2pix_project\\ENHANCED_20251225_203059\\checkpoints\\best.pth\n",
      "✓ New best model saved! (Val Loss: 11.9520)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 171/171 [05:44<00:00,  2.02s/it, D=2265.5813, G=13.0931, L1=0.0869, Perceptual=0.3710]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 - Val Loss: 11.3484 (L1: 0.0680, Perceptual: 0.3854)\n",
      "Checkpoint saved: ./enhanced_pix2pix_project\\ENHANCED_20251225_203059\\checkpoints\\best.pth\n",
      "✓ New best model saved! (Val Loss: 11.3484)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 171/171 [03:24<00:00,  1.20s/it, D=nan, G=nan, L1=0.0836, Perceptual=0.3592]         \n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 - Val Loss: nan (L1: 0.0668, Perceptual: 0.3700)\n",
      "Checkpoint saved: ./enhanced_pix2pix_project\\ENHANCED_20251225_203059\\checkpoints\\epoch_0005.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 171/171 [02:31<00:00,  1.13it/s, D=nan, G=nan, L1=0.1120, Perceptual=0.3943]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 - Val Loss: nan (L1: 0.0678, Perceptual: 0.3690)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 171/171 [02:35<00:00,  1.10it/s, D=nan, G=nan, L1=0.0950, Perceptual=0.3991]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 - Val Loss: nan (L1: 0.0680, Perceptual: 0.3707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 171/171 [06:06<00:00,  2.14s/it, D=nan, G=nan, L1=0.0894, Perceptual=0.3284]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 - Val Loss: nan (L1: 0.0674, Perceptual: 0.3692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 171/171 [07:50<00:00,  2.75s/it, D=nan, G=nan, L1=0.0971, Perceptual=0.3760]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 - Val Loss: nan (L1: 0.0674, Perceptual: 0.3741)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 171/171 [07:16<00:00,  2.55s/it, D=nan, G=nan, L1=0.0986, Perceptual=0.3808]\n",
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m EnhancedPix2PixTrainer(generator, discriminator, config)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Training completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 288\u001b[0m, in \u001b[0;36mEnhancedPix2PixTrainer.train\u001b[1;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mVAL_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 288\u001b[0m     avg_val_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m avg_val_losses\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m][k]\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[20], line 217\u001b[0m, in \u001b[0;36mEnhancedPix2PixTrainer.validate\u001b[1;34m(self, val_loader)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    215\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_GAN\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_L1\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_Perceptual\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m real_gray, real_rgb \u001b[38;5;129;01min\u001b[39;00m tqdm(val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    218\u001b[0m     real_gray, real_rgb \u001b[38;5;241m=\u001b[39m real_gray\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), real_rgb\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    220\u001b[0m     fake_rgb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(real_gray)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[19], line 103\u001b[0m, in \u001b[0;36mEnhancedPairedDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    100\u001b[0m seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2147483647\u001b[39m)\n\u001b[0;32m    102\u001b[0m gray_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(gray_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m rgb_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Set random seed for reproducible augmentation\u001b[39;00m\n\u001b[0;32m    106\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\pytorch_env\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 10: TRAIN THE MODEL\n",
    "# ===========================================\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = EnhancedPix2PixTrainer(generator, discriminator, config)\n",
    "\n",
    "# Start training\n",
    "trainer.train(train_loader, val_loader)\n",
    "\n",
    "print(\"\\n✓ Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: INFERENCE FUNCTION\n",
    "# ===========================================\n",
    "\n",
    "def run_enhanced_inference(checkpoint_path, input_image_path, output_dir='./enhanced_inference_results'):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    config_dict = checkpoint.get('config', {})\n",
    "    \n",
    "    # Create config\n",
    "    class InferenceConfig:\n",
    "        def __init__(self, config_dict):\n",
    "            for key, value in config_dict.items():\n",
    "                setattr(self, key, value)\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    inf_config = InferenceConfig(config_dict)\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = EnhancedGeneratorUNet(\n",
    "        in_channels=inf_config.IN_CHANNELS,\n",
    "        out_channels=inf_config.OUT_CHANNELS,\n",
    "        use_attention=getattr(inf_config, 'USE_ATTENTION', True),\n",
    "        use_se=getattr(inf_config, 'USE_SE_BLOCKS', True)\n",
    "    ).to(inf_config.device)\n",
    "    \n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded on {inf_config.device}\")\n",
    "    \n",
    "    # Prepare input\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((inf_config.IMG_SIZE, inf_config.IMG_SIZE)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    input_img = Image.open(input_image_path).convert('L')\n",
    "    input_tensor = transform(input_img).unsqueeze(0).to(inf_config.device)\n",
    "    \n",
    "    # Generate output\n",
    "    print(\"Generating colorized image...\")\n",
    "    with torch.no_grad():\n",
    "        output_tensor = generator(input_tensor)\n",
    "    \n",
    "    # Save output\n",
    "    output_img = output_tensor.squeeze(0).cpu()\n",
    "    output_img = (output_img * 0.5 + 0.5).clamp(0, 1)\n",
    "    output_img = transforms.ToPILImage()(output_img)\n",
    "    \n",
    "    input_name = os.path.splitext(os.path.basename(input_image_path))[0]\n",
    "    output_path = os.path.join(output_dir, f'{input_name}_colorized.png')\n",
    "    output_img.save(output_path)\n",
    "    \n",
    "    # Save input for comparison\n",
    "    input_img.save(os.path.join(output_dir, f'{input_name}_input.png'))\n",
    "    \n",
    "    # Create side-by-side comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(input_img, cmap='gray')\n",
    "    axes[0].set_title('Input (Grayscale)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(output_img)\n",
    "    axes[1].set_title('Generated (RGB)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    comparison_path = os.path.join(output_dir, f'{input_name}_comparison.png')\n",
    "    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n✓ Results saved:\")\n",
    "    print(f\"  - Input: {os.path.join(output_dir, f'{input_name}_input.png')}\")\n",
    "    print(f\"  - Output: {output_path}\")\n",
    "    print(f\"  - Comparison: {comparison_path}\")\n",
    "    \n",
    "    return output_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH INFERENCE FOR EVENT HALL IMAGES\n",
    "\n",
    "def batch_inference(checkpoint_path, input_dir, output_dir='./batch_colorized_results', max_images=None):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp')\n",
    "    image_files = [f for f in os.listdir(input_dir) \n",
    "                   if f.lower().endswith(image_extensions)]\n",
    "    \n",
    "    if max_images:\n",
    "        image_files = image_files[:max_images]\n",
    "    \n",
    "    print(f\"\\nProcessing {len(image_files)} images from {input_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load model once\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    config_dict = checkpoint.get('config', {})\n",
    "    \n",
    "    class InferenceConfig:\n",
    "        def __init__(self, config_dict):\n",
    "            for key, value in config_dict.items():\n",
    "                setattr(self, key, value)\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    inf_config = InferenceConfig(config_dict)\n",
    "    \n",
    "    generator = EnhancedGeneratorUNet(\n",
    "        in_channels=inf_config.IN_CHANNELS,\n",
    "        out_channels=inf_config.OUT_CHANNELS,\n",
    "        use_attention=getattr(inf_config, 'USE_ATTENTION', True),\n",
    "        use_se=getattr(inf_config, 'USE_SE_BLOCKS', True)\n",
    "    ).to(inf_config.device)\n",
    "    \n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((inf_config.IMG_SIZE, inf_config.IMG_SIZE)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # Process images\n",
    "    for img_file in tqdm(image_files, desc=\"Colorizing images\"):\n",
    "        try:\n",
    "            input_path = os.path.join(input_dir, img_file)\n",
    "            input_img = Image.open(input_path).convert('L')\n",
    "            input_tensor = transform(input_img).unsqueeze(0).to(inf_config.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output_tensor = generator(input_tensor)\n",
    "            \n",
    "            output_img = output_tensor.squeeze(0).cpu()\n",
    "            output_img = (output_img * 0.5 + 0.5).clamp(0, 1)\n",
    "            output_img = transforms.ToPILImage()(output_img)\n",
    "            \n",
    "            # Save output\n",
    "            output_name = os.path.splitext(img_file)[0] + '_colorized.png'\n",
    "            output_path = os.path.join(output_dir, output_name)\n",
    "            output_img.save(output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✓ Batch inference complete!\")\n",
    "    print(f\"✓ Colorized images saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: EXAMPLE USAGE\n",
    "# ===========================================\n",
    "\n",
    "# AFTER TRAINING IS COMPLETE, use these functions for inference:\n",
    "\n",
    "# Example 1: Single image inference\n",
    "# run_enhanced_inference(\n",
    "#     checkpoint_path='./enhanced_pix2pix_project/ENHANCED_YYYYMMDD_HHMMSS/checkpoints/best.pth',\n",
    "#     input_image_path='path/to/your/grayscale/image.jpg',\n",
    "#     output_dir='./colorized_results'\n",
    "# )\n",
    "\n",
    "# Example 2: Batch inference on all 5000 event hall images\n",
    "# batch_inference(\n",
    "#     checkpoint_path='./enhanced_pix2pix_project/ENHANCED_YYYYMMDD_HHMMSS/checkpoints/best.pth',\n",
    "#     input_dir='pix2pix_dataset/test/gray',\n",
    "#     output_dir='./all_colorized_results',\n",
    "#     max_images=None  # Process all images\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
